---
title: 'PML: Course Project'
author: "Yuri Isakov"
date: "28 January 2016"
output: md_document
---

### Obtaining data

Training and test set was downloaded into working directory. Then empty columns were excluded. Actually it could be done in more elegant way during loading csv files.
```{r}
train <- read.csv("pml-training.csv", stringsAsFactors = FALSE)
test <- read.csv("pml-testing.csv", stringsAsFactors = FALSE)
minusCol1 <- colSums(train == "") > 10000
minusCol1[is.na(minusCol1)]  <- TRUE
minusCol2 <- colSums(is.na(train)) > 10000
minusCol <- minusCol1 | minusCol2
minusCol[c(1, 3:6)] <- TRUE
train <- train[,!minusCol]
```

#### Using `user_name` variable
I know some students have opinions that it's not necessary to use ID variable. I believe different persons can do things in different ways. That's why I didn't omit this variable. Here's a plot that shows my motivation. For some users parameters may be significant different.

```{r cache=TRUE, message=FALSE}
library(ggplot2)
qplot(train[,1], train[,2], colour = train$classe)
```

### Gradient boosting method
Gradient boosting method showed excellent results but it took pretty long time to train model (more than 30 minutes). Accuracy on training set is higher 99%. This model gave 20/20 answers on final test.
```{r cache=TRUE, message=FALSE}
library(caret)
# fit_gbm <- train(classe ~., data = tr, method = "gbm")
fit_gbm <- readRDS("gbm01.rds")
confusionMatrix(predict(fit_gbm, train), train$classe)
```

### Random forests
Default settings of training model using random forests causing leads to long training time. Using cross validation tuning it's possible to reduce training time. Parameter `number` divides training set on 2 folds in my case (while default settings have 25 resampling iterations). This gives 100% accuracy on training set and 20/20 on test set. Training time was about 6 minutes and it increases by 5-6 minutes with every new fold.

```{r}
library(caret)
# set.seed(19)
# fit_rf2 <- train(classe ~., 
#                 method="rf", 
#                 trControl=trainControl(method = "cv",
#                                        number = 2), 
#                 data=train)
fit_rf2 <- readRDS("fit_rf2.rds")
confusionMatrix(predict(fit_rf2, train), train$classe)
```

### Conclusion
Both methods, Gradient Boosting and Random Forests gave excellent results on training set with accuracy >99% and 100% respectively. Both methods passed final test 20/20. Full code and models available on my repo: https://github.com/yurkai/PML-CP

Ps. To my mind this project was controversial. Because I used really sophisticated and complex methods just with few lines of code without understanding of inner mechanics of these methods. If you'll say that I better spend time on it and there are tons of information you will probably right, that's why took another few courses dedicated machine and statistical learning. Thanks.